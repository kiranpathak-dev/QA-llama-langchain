{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIJywztO+hMo+2CYvLI9wI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ea80c4ff5a4246519d62b505599f94a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06e260d06ba44b4b911e910278b04fdd",
              "IPY_MODEL_a126d52328cc4642aa5ba8a292f47a18",
              "IPY_MODEL_4d729677b9a0413a92cb2cc12e07fc51"
            ],
            "layout": "IPY_MODEL_2c8984d96bc24584b64682ba29ff0a2b"
          }
        },
        "06e260d06ba44b4b911e910278b04fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9054a910baf471daf69cefc678f5f5b",
            "placeholder": "​",
            "style": "IPY_MODEL_fcc1da18917444518574b1ab4b9de1d8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a126d52328cc4642aa5ba8a292f47a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbea4d6f304946c2a0992c070bbf0bbd",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_126bae8d93cc485794e2e089da506306",
            "value": 2
          }
        },
        "4d729677b9a0413a92cb2cc12e07fc51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74975182aa654a99953e0bc8190626c2",
            "placeholder": "​",
            "style": "IPY_MODEL_7380de39405a4bcda3d73420fb8f788e",
            "value": " 2/2 [01:03&lt;00:00, 28.90s/it]"
          }
        },
        "2c8984d96bc24584b64682ba29ff0a2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9054a910baf471daf69cefc678f5f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc1da18917444518574b1ab4b9de1d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbea4d6f304946c2a0992c070bbf0bbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "126bae8d93cc485794e2e089da506306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74975182aa654a99953e0bc8190626c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7380de39405a4bcda3d73420fb8f788e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiranpathak-dev/QA-llama-langchain/blob/main/Question_Answering_on_Your_Own_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data\n",
        "Question-Answering (QA) chatbot using Llama-2–7b-chat model with LangChain framework and FAISS library over the documents fetched online from Databricks documentation website."
      ],
      "metadata": {
        "id": "1FmHHqQfQzqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LLaMA 2 model is one of the powerful open source models.\n",
        "* LangChain is a powerful, open-source framework designed to help you develop applications powered by a language model, particularly a large language model (LLM).\n",
        "* FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors."
      ],
      "metadata": {
        "id": "rU7GyxURRQVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Flow\n",
        "* **Initialize model pipeline**: initializing text-generation pipeline with Hugging Face transformers for the pretrained Llama-2-7b-chat-hf model.\n",
        "\n",
        "* **Ingest data**: loading the data from arbitrary sources in the form of text into the document loader.\n",
        "\n",
        "* **Split into chunks**: splitting the loaded text into smaller chunks. It is necessary to create small chunks of text because language models can handle limited amount of text.\n",
        "\n",
        "* **Create embeddings**: converting the chunks of text into numerical values, also known as embeddings. These embeddings are used to search and retrieve similar or relevant documents quickly in large databases, as they represent the semantic meaning of the text.\n",
        "\n",
        "* **Load embeddings into vector store**: loading the embeddings into a vector store i.e. “FAISS” in this case. Vector stores perform extremely well in similarity search using text embeddings compared to the traditional databases.\n",
        "\n",
        "* **Enable memory**: combing chat history with a new question and turn them into a single standalone question is quite important to enable the ability to ask follow up questions.\n",
        "\n",
        "* **Query data**: searching for the relevant information stored in vector store using the embeddings.\n",
        "\n",
        "* **Generate answer**: passing the standalone question and the relevant information to the question-answering chain where the language model is used to generate an answer."
      ],
      "metadata": {
        "id": "FWvyIlU7SOmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "You can use the open source Llama-2-7b-chat model in both Hugging Face transformers and LangChain. However, you have to first request access to Llama 2 models via [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads) and also accept to share your account details with Meta on [Hugging Face website](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). It typically takes a few minutes or hours to get the access."
      ],
      "metadata": {
        "id": "F5h3egh0TDbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚨 Note that your Hugging Face account email MUST match the email you provided on the Meta website, or your request will not be approved."
      ],
      "metadata": {
        "id": "W0yw7e0fTuat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you’re using Google Colab to run the code. In your notebook, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4. You will need ~8GB of GPU RAM for inference and running on CPU is practically impossible."
      ],
      "metadata": {
        "id": "aJzEBxgoTxX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the Libraries\n",
        "First of all, let’s start by installing all required libraries using pip install."
      ],
      "metadata": {
        "id": "KZDVGAWwT1Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate==0.21.0 transformers==4.31.0 tokenizers==0.13.3\n",
        "!pip install bitsandbytes==0.40.0 einops==0.6.1\n",
        "!pip install xformers==0.0.22.post7\n",
        "!pip install langchain==0.1.4\n",
        "!pip install faiss-gpu==1.7.1.post3\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIAik0kkRQFb",
        "outputId": "de28f20e-4712-4dca-c83b-d1d5617044ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.21.0 in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: tokenizers==0.13.3 in /usr/local/lib/python3.10/dist-packages (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.20.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes==0.40.0 in /usr/local/lib/python3.10/dist-packages (0.40.0)\n",
            "Requirement already satisfied: einops==0.6.1 in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: xformers==0.0.22.post7 in /usr/local/lib/python3.10/dist-packages (0.0.22.post7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.22.post7) (1.25.2)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.22.post7) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->xformers==0.0.22.post7) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->xformers==0.0.22.post7) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->xformers==0.0.22.post7) (1.3.0)\n",
            "Requirement already satisfied: langchain==0.1.4 in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.0.20)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.1.23)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (0.0.87)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.4) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.4) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.4) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.4) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.4) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.4) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.4) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.4) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.4) (1.2.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.4) (1.0.0)\n",
            "Requirement already satisfied: faiss-gpu==1.7.1.post3 in /usr/local/lib/python3.10/dist-packages (1.7.1.post3)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Collecting transformers<5.0.0,>=4.34.0 (from sentence_transformers)\n",
            "  Using cached transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2023.12.25)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
            "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.31.0\n",
            "    Uninstalling transformers-4.31.0:\n",
            "      Successfully uninstalled transformers-4.31.0\n",
            "Successfully installed tokenizers-0.19.1 transformers-4.40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the Hugging Face Pipeline\n",
        "You have to initialize a text-generation pipeline with Hugging Face transformers. The pipeline requires the following three things that you must initialize:\n",
        "\n",
        "* A LLM, in this case it will be meta-llama/Llama-2-7b-chat-hf.\n",
        "*  respective tokenizer for the model.\n",
        "* A stopping criteria object.\n",
        "\n",
        "You have to initialize the model and move it to CUDA-enabled GPU. Using Colab, this can take 5–10 minutes to download and initialize the model.\n",
        "\n",
        "Also, you need to generate an access token to allow downloading the model from Hugging Face in your code. For that, go to your Hugging Face Profile > Settings > Access Token > New Token > Generate a Token. Just copy the token and add it in the below code."
      ],
      "metadata": {
        "id": "noETDj9aUonp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "ea80c4ff5a4246519d62b505599f94a9",
            "06e260d06ba44b4b911e910278b04fdd",
            "a126d52328cc4642aa5ba8a292f47a18",
            "4d729677b9a0413a92cb2cc12e07fc51",
            "2c8984d96bc24584b64682ba29ff0a2b",
            "c9054a910baf471daf69cefc678f5f5b",
            "fcc1da18917444518574b1ab4b9de1d8",
            "dbea4d6f304946c2a0992c070bbf0bbd",
            "126bae8d93cc485794e2e089da506306",
            "74975182aa654a99953e0bc8190626c2",
            "7380de39405a4bcda3d73420fb8f788e"
          ]
        },
        "id": "ElIQXeJ3QsCD",
        "outputId": "30b3935e-74a9-424b-f981-6fb571c64662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py:913: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea80c4ff5a4246519d62b505599f94a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, you need an access token\n",
        "hf_auth = 'add your access token here'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "# enable evaluation mode to allow model inference\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline requires a tokenizer which handles the translation of human readable plaintext to LLM readable token IDs. The Llama 2 7B models were trained using the Llama 2 7B tokenizer, which can be initialized with this code:"
      ],
      "metadata": {
        "id": "qQBpe6ttVHIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SahKLNBlVAxo",
        "outputId": "42165b91-5d92-4538-8c64-91751263156f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:757: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to define the stopping criteria of the model. The stopping criteria allows us to specify when the model should stop generating text. If we don’t provide a stopping criteria the model just goes on a bit tangent after answering the initial question."
      ],
      "metadata": {
        "id": "oWdQ7rDCVeni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izD5ryB6Vgrh",
        "outputId": "da3b7396-868d-40e1-a1ae-656bb71dc8e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to convert these stop token ids into LongTensor objects."
      ],
      "metadata": {
        "id": "4RU6RF3gWTq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qNwHqFmWVFe",
        "outputId": "c9a287b0-7204-4006-9227-3f05215440ac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
              " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can do a quick spot check that no <unk> token IDs (0) appear in the stop_token_ids — there are none so we can move on to building the stopping criteria object that will check whether the stopping criteria has been satisfied — meaning whether any of these token ID combinations have been generated."
      ],
      "metadata": {
        "id": "WlWcsroeWe_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "fVvjcom6WXF8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are ready to initialize the Hugging Face pipeline. There are a few additional parameters that we must define here."
      ],
      "metadata": {
        "id": "lCjxrITrW59o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "nfvE1fs7W36Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate_text(\"Explain me the difference between Data Lakehouse and Data Warehouse.\")\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abQWMFO7Wj1K",
        "outputId": "cb28bfba-5f84-4e9c-af8f-3c044d4dfe8b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain me the difference between Data Lakehouse and Data Warehouse. Unterscheidung between data lakehouse and data warehouse is a common topic of discussion in the data engineering community, as both are designed to store large amounts of data but have different architectures and use cases. A data lakehouse is a centralized repository that stores all the raw data from various sources in its original form, while a data warehouse is a structured repository that organizes data into a specific schema for querying and analysis.\n",
            "\n",
            "A data lakehouse is a centralized repository that stores all the raw data from various sources in its original form. This means that the data is not transformed or cleaned before being stored in the lakehouse, which allows for greater flexibility in how the data can be used. In contrast, a data warehouse is a structured repository that organizes data into a specific schema for querying and analysis. The data in a data warehouse is typically transformed and cleaned before being stored, which makes it easier to query and analyze but limits the types of queries that can be run.\n",
            "\n",
            "One key advantage of a data lakehouse is that it allows for more flexible and ad-hoc querying and analysis of the data. Because the data is stored in its original form, users can access the data directly and perform complex queries without having to transform or clean the data first. In contrast, a data warehouse is optimized for fast query performance, which means that users must follow a predefined schema when querying the data.\n",
            "\n",
            "Another advantage of a data lakehouse is that it can handle large volumes of unstructured data, such as images, videos, and text documents. This is because a data lakehouse does not require data to be transformed or cleaned before being stored, which makes it well-suited for storing and processing big data. In contrast, a data warehouse is typically designed to handle structured data, such as relational databases, and may struggle with handling large volumes of unstructured data.\n",
            "\n",
            "However, there are also some limitations to using a data lakehouse. One limitation is that it can be more difficult to manage and maintain data quality in a data lakehouse than in a data warehouse. Because the data is not transformed or cleaned before being stored, it may contain errors or inconsistencies that need to be addressed before it can be used for analysis. Additionally, a data lakehouse may require more advanced technical skills to set up and manage compared to a data warehouse.\n",
            "\n",
            "In\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing HF Pipeline in LangChain\n",
        "\n",
        "Now, you have to implement the Hugging Face pipeline in LangChain. You will still get the same output as nothing different is being done here. However, this code will allow you to use LangChain’s advanced agent tooling, chains, etc, with Llama 2."
      ],
      "metadata": {
        "id": "KJInQUHmaNQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "# checking again that everything is working fine\n",
        "llm(prompt=\"Explain me the difference between Data Lakehouse and Data Warehouse.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "um2M9OyaXcer",
        "outputId": "67a64e79-1808-4154-9e98-ef44d6e27b7d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Unterscheidung between data lakehouse and data warehouse is a common topic of discussion in the data engineering community, as both are designed to store large amounts of data but have different architectures and use cases. A data lakehouse is a centralized repository that stores all the raw data from various sources in its original form, without transforming or processing it. On the other hand, a data warehouse is a structured repository that stores data in a specific format, typically after cleaning, transforming, and aggregating it.\\n\\nHere are some key differences between a data lakehouse and a data warehouse:\\n\\n1. Data Structure: A data lakehouse stores data in its raw form, without any predefined schema or structure. In contrast, a data warehouse stores data in a structured format, with a defined schema that defines the relationships between different data entities.\\n2. Data Processing: A data lakehouse does not process or transform data before storing it. Instead, it relies on downstream tools and systems to perform data processing and analysis. In contrast, a data warehouse performs data transformation and cleansing before storing it.\\n3. Scalability: A data lakehouse can scale horizontally by adding more nodes to the cluster, making it easier to handle large volumes of data. A data warehouse, on the other hand, can become bottlenecked as the volume of data grows, requiring more expensive hardware upgrades.\\n4. Data Governance: A data lakehouse provides better data governance compared to a data warehouse since it allows for more granular access control and auditing capabilities.\\n5. Cost: A data lakehouse is generally less expensive than a data warehouse due to its distributed architecture and lack of predefined schema.\\n6. Use Cases: A data lakehouse is ideal for use cases that require handling large volumes of unstructured or semi-structured data, such as IoT sensor data, social media feeds, or log files. In contrast, a data warehouse is better suited for use cases that require complex queries and aggregations, such as sales forecasting or customer segmentation.\\n7. Query Performance: A data lakehouse can provide faster query performance compared to a data warehouse due to its distributed architecture and ability to parallelize queries across multiple nodes.\\n8. Data Integration: A data lakehouse provides better support for data integration compared'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingesting Data using Document Loader\n",
        "\n",
        "You have to ingest data using WebBaseLoader document loader which collects data by scraping webpages. In this case, you will be collecting data from Databricks documentation website."
      ],
      "metadata": {
        "id": "LSmjMP1VhDTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "web_links = [\"https://www.databricks.com/\",\"https://help.databricks.com\",\"https://databricks.com/try-databricks\",\"https://help.databricks.com/s/\",\"https://docs.databricks.com\",\"https://kb.databricks.com/\",\"http://docs.databricks.com/getting-started/index.html\",\"http://docs.databricks.com/introduction/index.html\",\"http://docs.databricks.com/getting-started/tutorials/index.html\",\"http://docs.databricks.com/release-notes/index.html\",\"http://docs.databricks.com/ingestion/index.html\",\"http://docs.databricks.com/exploratory-data-analysis/index.html\",\"http://docs.databricks.com/data-preparation/index.html\",\"http://docs.databricks.com/data-sharing/index.html\",\"http://docs.databricks.com/marketplace/index.html\",\"http://docs.databricks.com/workspace-index.html\",\"http://docs.databricks.com/machine-learning/index.html\",\"http://docs.databricks.com/sql/index.html\",\"http://docs.databricks.com/delta/index.html\",\"http://docs.databricks.com/dev-tools/index.html\",\"http://docs.databricks.com/integrations/index.html\",\"http://docs.databricks.com/administration-guide/index.html\",\"http://docs.databricks.com/security/index.html\",\"http://docs.databricks.com/data-governance/index.html\",\"http://docs.databricks.com/lakehouse-architecture/index.html\",\"http://docs.databricks.com/reference/api.html\",\"http://docs.databricks.com/resources/index.html\",\"http://docs.databricks.com/whats-coming.html\",\"http://docs.databricks.com/archive/index.html\",\"http://docs.databricks.com/lakehouse/index.html\",\"http://docs.databricks.com/getting-started/quick-start.html\",\"http://docs.databricks.com/getting-started/etl-quick-start.html\",\"http://docs.databricks.com/getting-started/lakehouse-e2e.html\",\"http://docs.databricks.com/getting-started/free-training.html\",\"http://docs.databricks.com/sql/language-manual/index.html\",\"http://docs.databricks.com/error-messages/index.html\",\"http://www.apache.org/\",\"https://databricks.com/privacy-policy\",\"https://databricks.com/terms-of-use\"]\n",
        "\n",
        "loader = WebBaseLoader(web_links)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "a7q5Wf0GhDAH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting in Chunks using Text Splitters**\n",
        "\n",
        "You have to make sure to split the text into small pieces. You will need to initialize RecursiveCharacterTextSplitter and call it by passing the documents.\n"
      ],
      "metadata": {
        "id": "vUoCUDY8h2Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "VyNKY7ape9YB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Embeddings and Storing in Vector Store**\n",
        "\n",
        "You have to create embeddings for each small chunk of text and store them in the vector store (i.e. FAISS). You will be using all-mpnet-base-v2 Sentence Transformer to convert all pieces of text in vectors while storing them in the vector store"
      ],
      "metadata": {
        "id": "MdNudsQYiFKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "# storing embeddings in the vector store\n",
        "vectorstore = FAISS.from_documents(all_splits, embeddings)"
      ],
      "metadata": {
        "id": "SdZq4wDNiCNC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing Chain**\n",
        "\n",
        "You have to initialize `ConversationalRetrievalChain`. This chain allows you to have a chatbot with memory while relying on a vector store to find relevant information from your document."
      ],
      "metadata": {
        "id": "0xYGJr9Gix_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, you can return the source documents used to answer the question by specifying an optional parameter i.e. `return_source_documents=True` when constructing the chain."
      ],
      "metadata": {
        "id": "74_vo-l3jQ6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
      ],
      "metadata": {
        "id": "4y_bqdy2jP3G"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, it’s time to do some Question-Answering on your own data!"
      ],
      "metadata": {
        "id": "LUDqaGAQjnYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "query = \"What is Data lakehouse architecture in Databricks?\"\n",
        "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54uBsQOrjOzZ",
        "outputId": "653a7361-d9c1-4f7b-ace6-fb3903d561e3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In Databricks, a data lakehouse architecture refers to a scalable storage and processing system that supports various workloads, such as machine learning and business intelligence. It consists of multiple layers, including a bronze, silver, and gold layer, each providing increasing levels of data quality and refinement. The medallion architecture is a specific data design pattern used in lakehouses, which involves incrementally improving, enriching, and refining data as it moves through layers of staging and transformation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time your previous question and answer will be included as a chat history which will enable the ability to ask follow up questions."
      ],
      "metadata": {
        "id": "9rmo1Ttovs_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [(query, result[\"answer\"])]\n",
        "\n",
        "query = \"What are Data Governance and Interoperability in it?\"\n",
        "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpNal-7Avsj1",
        "outputId": "0bc849e6-e403-4dd6-8661-0760ea613d6b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In a data lakehouse architecture, data governance refers to the policies, procedures, and standards that manage data quality, security, access, and usage across various stakeholders. Unlike traditional data governance practices, which focus primarily on data quality and security within a specific organization, data governance in a data lakehouse architecture must consider the needs of multiple stakeholders, including external parties such as customers and partners. Additionally, data governance in a data lakehouse architecture must accommodate the decentralized nature of data, which may reside in various locations and be owned by different entities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also see the source of the information used to generate the answer."
      ],
      "metadata": {
        "id": "TDXJ-fkVv5cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['source_documents'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4X63VAriVT2",
        "outputId": "90a7a177-e192-49db-b364-de75c5896506"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Administration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance (Unity Catalog)\\nLakehouse architecture\\nScope of the lakehouse\\nGuiding principles\\nReference architecture\\nThe well-architected lakehouse\\n\\n\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Apr 23, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nIntroduction to the well-architected data lakehouse\\n\\n\\n\\n\\n\\n\\n\\nIntroduction to the well-architected data lakehouse \\nAs a cloud architect, when you evaluate a data lakehouse implementation on the Databricks Data Intelligence Platform, you might want to know “What is a good lakehouse?” The Well-architected lakehouse articles provide guidance for lakehouse implementation.\\nAt the outset, you might also want to know:\\n\\nWhat is the scope of the lakehouse - in terms of capabilities and personas?\\nWhat is the vision for the lakehouse?\\nHow does the lakehouse integrate with the customer’s cloud architecture?', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Introduction to the well-architected data lakehouse | Databricks on AWS', 'description': 'Introduction to articles that describe principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}), Document(page_content='Administration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance (Unity Catalog)\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Apr 23, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is data warehousing on Databricks?\\n\\n\\n\\n\\n\\n\\n\\nWhat is data warehousing on Databricks? \\nData warehousing refers to collecting and storing data from multiple sources so it can be quickly accessed for business insights and reporting. This article contains key concepts for building a data warehouse in your data lakehouse.\\n\\nData warehousing in your lakehouse \\nThe lakehouse architecture and Databricks SQL bring cloud data warehousing capabilities to your data lakes. Using familiar data structures, relations, and management tools, you can model a highly-performant, cost-effective data warehouse that runs directly on your data lake. For more information, see What is a data lakehouse?', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}), Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance (Unity Catalog)\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Apr 23, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is Databricks? \\nWhat is a data lakehouse?\\n\\n\\n\\n\\n\\n\\n\\nWhat is a data lakehouse? \\nA data lakehouse is a data management system that combines the benefits of data lakes and data warehouses. This article describes the lakehouse architectural pattern and what you can do with it on Databricks.', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}), Document(page_content='What is a data lakehouse used for? \\nA data lakehouse provides scalable storage and processing capabilities for modern organizations that want to avoid isolated systems for processing different workloads, like machine learning (ML) and business intelligence (BI). A data lakehouse can help establish a single source of truth, eliminate redundant costs, and ensure data freshness.\\nData lakehouses often use a data design pattern that incrementally improves, enriches, and refines data as it moves through layers of staging and transformation. Each layer of the lakehouse can include one or more layers. This pattern is frequently referred to as a medallion architecture. For more information, see What is the medallion lakehouse architecture?', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally…\n",
        "Et voilà! You have now the capability to do question-answering on your on data using a powerful language model. Additionally, we can further develop it into a chatbot application using Streamlit."
      ],
      "metadata": {
        "id": "SnULUvbqxDCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resource: [\"Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data](https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476)\""
      ],
      "metadata": {
        "id": "BByAp_Gs3fQG"
      }
    }
  ]
}